Data Engineering Test


Jesús Manuel España Tzec


Part I: Technical Questions


1. What is Data Engineering?
Data Engineering is the task of designing and building automated systems or flows for extracting, wrangling, and storing data to be analyzed and used to make decisions.

2. What are the main responsibilities of a Data Engineer?
The main responsibilities are managing and converting raw data into useful information.

3. Explain ETL.
ETL stands for Extract Load and Transform, this is the process where it collects data from various sources, 
then transforms the data according to business rules and finally it loads the data to a destination data store like a data warehouse. 
For example, imagine that you buy three lego toys from different stores (Extract), then you build completely your three lego toys (Transform) and finally you put them in the bookcase of your room (Load).

4. How you build a Data Pipeline? Feel free to explain with a fictional example.
The first thing to think about when building a Data Pipeline is what kind of Data Pipeline we need, a batch Data Pipeline or stream Data Pipeline. 
In this case I’ll use a batch Data Pipeline, suppose that we need move data from sales of different products, this data is ingested daily at 4 am in a Data Lake, 
now we know the periodicity of the data and where we have to collect it we can build the first step that is extract this raw data from the data lake, 
the second step is analyzed what kind of cleaning and transformations is needed. For example, if we just need the data of a small group of products, 
filter the data to only move this data, ensure that the different columns have its correct data type and create some calculations with some columns if it's needed. 
And finally, we can ingest this processed data to the destination data store where it will be consumed by others.

5. In a RDBMS, the Join command is your friend. Explain why.
The reason is because in a RDBMS we have relations that organize the data, so we have tables with data that we can join to access the information we are interested in. 
For example, in a bank's RDBMS, it can have a table with the data of its clients and another table with the data of transactions, 
so we can join these two tables to access the data of the transactions of each client. 

6. What are the main features of a production pipeline?
The main features are that it is scheduled to run at a specific periodicity, the pipeline can deal with different volumes of data, 
it can guarantee the reliability of the data and that the pipeline can handle errors.

7. How do you monitor a data pipeline?
Of course you can monitor a Data Pipeline through the GUI if your tool supports it, but a better way to monitor a Data Pipeline is through the logs to see how it is working.

8. Give us a situation where you decide to use a NoSQL database instead of a relational database and explain why.
A situation could be for storing media for a social media platform, this media could be images, videos, sound files for example, 
and the reason is because the data come in different shapes and sizes and needs to be available fast.

9. What are the non-technical soft skills that are most valuable for data engineers?
For me, the most valuable soft skills are collaboration to be able to work with different persons in different projects, 
communication to be able to explain your work or problems you are facing  and critical thinking to analyze the data you are working with and the business needs.

10. Suppose you have to design an Anomaly Detection Solution for a client in real or near real time. A platform for anomaly detection is about finding patterns of interest (outliers, exceptions, peculiarities, etc.) that deviate from expected behavior within dataset(s). Given this definition, it’s worth noting that anomaly detection is, therefore, very similar to noise removal and novelty detection. Though patterns detected with anomaly detection are actually of interest, noise detection can be slightly different because the sole purpose of detection is removing those anomalies - or noise - from data. Which technologies do you apply for real time ingestion and stream for an anomaly detection system? Diagram the solution in AWS or GCP Infrastructure.

11. Explain the differences between OLAP and OLTP Systems.
The first one is the focus, while OLAP systems are for performing analysis on large volumes of data OLTP systems are for processing large volumes of transactional data. 
Second one is the speed because OLAP systems have to process considerably more data than OLTP systems. And the last difference is the data source, 
OLAP systems can handle multi-dimensional schemas which means it can support complex queries, OLTP systems consumes from transactional databases.